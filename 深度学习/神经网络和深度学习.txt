一共5课
	如何建立并运用一个深度神经网络
第一周: 入门
	正向传播和反向传播的结构
	logistic回归
	算法的过程
	如何高效的实现神经网络
	
第二周: 神经网络的编程基础
	编程练习
	自己实现算法, 亲自调试到完美运行
	
第三周:
	会编写单隐层神经网络
	关键概念
第四周:
	建立一个多层的深层神经网络
		
▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
				神经网络基础
▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
二分分类


▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
logistic回归函数
	是一个学习算法, 用在监督学习问题中, 输出y标签是0或者1时
	是一个二分分类问题
// https://blog.csdn.net/alw_123/article/details/82193535
回归: 	预测值是一个连续值
离散值: 分类
线性回归: 在N维空间中找一个形式像直线方程一样的函数来拟合数据	// 直线的预测是一个连续值
损失函数: 用来量化预测结果和真实结果的误差的一个函数。
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
https://blog.csdn.net/qq_32113189/article/details/79908825

logistic回归损失函数
	损失函数: 用来量化预测结果和真实结果的误差的一个函数。
	它衡量了在单个训练样本上的表现
	L(y^(i),y(i)) = (y^(i)−y(i))^2 / 2 
	L(y^(i),y(i)) = −(y(i)log(y^(i)) + (1−y(i))log(1−y^(i)))
成本函数: 
	1.全体训练样本上的表现	
	2.成本函数是整个训练集损失函数的平均值。我们试着去找到参数w和b使得成本函数最小化。
	3.成本函数是一个凸曲面函数
	J(w,b)=[∑(m,i=1) L(y^(i),y(i))] /m = [∑(m,i=1)(y(i))log(y^(i))+(1−y(i))log(1−y^(i))] /m

nx表示特征向量的维度 = 元素个数
二分类问题:
	目的是训练出一个分类器，他一图片的特征向量ｘ作为输入，预测输出的是１还是０
约定
	（x,y）: 表示一个单独的样本
	x	:	是nx维的特征向量	
	y的值域: 是{0,1}	
	训练集: 由m个训练样本组成
	m: 训练集的样本个数
	(x^1, y^1): 表示样本1的输入输出
	Mtest: 测试集中的样本个数
	nx: 表示矩阵的高度,也就是单样本输入的参数个数
	用X来表示训练集的输入,他由x^1,x^2...x^m组成
			第	第		第
			一	二	...	m	
			个	个		个
			样	样		样
		X=	本	本		本	
			的	的		的	
			输	输		输	
			入	入		入	
			x^1 x^2		x^m	
		这个矩阵有m列, nx行
	X.shape表示矩阵X的维度
	用Y来表示训练集的结果
			第	第		第
			一	二	...	m	
			个	个		个
			样	样		样
		Y=	本	本		本	
			的	的		的	
			输	输		输	
			出	出		出	
			y^1 y^2		y^m	
	Y表示输出中变量的个数		
			
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
梯度下降法
			从初始点开始,槽最陡的下坡方向走一步
		步长（Learning rate）：
			步长又称学习率，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。
			用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。
	2.3 梯度下降法调优
		1.在梯度下降法中调优比较重要的是3个因素，步长、初始值、归一化。
		2.步长：步长太小，收敛慢，步长太大，会远离最优解。所以需要从小到大，分别测试，选出一个最优解。
		3.初始值：随机选取初始值，当损失函数是非凸函数时，找到的解可能是局部最优解，需要多测试几次，从局部最优解中选出最优解。当损失函数是凸函数时，得到的解就是最优解。
		4.归一化：如果不归一化，会收敛的很慢，会形成之字的路线。
		5.函数的斜率,是朝着下降熟读最快的方向走: w = w - dJ(w)/dw
		w = w - dJ(w,b)/dw
		b = b - dJ(w,b)/db
	方向导数与梯度：
		1.导数反应的是函数的变化率
		2.而偏导数反应的是函数沿坐标轴方向的变化率
		3.偏导数 f'x(x0,y0) 表示固定面上一点对 x 轴的切线斜率；偏导数 f'y(x0,y0) 表示固定面上一点对 y 轴的切线斜率。
	导数就是斜率	
	链式法则：
		a影响到v，v影响到j: dj/da = dj/dv * dv/da
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
向量化	
	用Python里面的numpy内置函数来代替for循环,因为for循环非常耗时
	np.function	// 内置函数
	1.
		z^1 = w^Tx^i +b
		Z = [z^1, z^2,...z,^m]
		  = [w^Tx^1 +b, w^Tx^2 +b,...,w^Tx^m +b]
		  = w^T[x^1, ...,x^m] + [b,...,b]
		  = w^TX + [b,...,b]	
		python中写为: 
		  Z = np.dot(w.T,X) +b; //当矩阵+常数时,python会自动将常数转换为对应的矩阵
	2.
		dZ = [dz^1,...,dz^m]
		dZ的元素之和在Python中表示为np.sum(dZ)




