一共5课
	如何建立并运用一个深度神经网络
第一周: 入门
	正向传播和反向传播的结构
	logistic回归
	算法的过程
	如何高效的实现神经网络
	
第二周: 神经网络的编程基础
	编程练习
	自己实现算法, 亲自调试到完美运行
	
第三周:
	会编写单隐层神经网络
	关键概念
第四周:
	建立一个多层的深层神经网络
		
▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
				神经网络基础
▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂
二分分类


▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
logistic回归函数
	是一个学习算法, 用在监督学习问题中, 输出y标签是0或者1时
	是一个二分分类问题
// https://blog.csdn.net/alw_123/article/details/82193535
回归: 	预测值是一个连续值
离散值: 分类
线性回归: 在N维空间中找一个形式像直线方程一样的函数来拟合数据	// 直线的预测是一个连续值
损失函数: 用来量化预测结果和真实结果的误差的一个函数。
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
https://blog.csdn.net/qq_32113189/article/details/79908825

logistic回归损失函数
	损失函数: 用来量化预测结果和真实结果的误差的一个函数。
	它衡量了在单个训练样本上的表现
	L(y^(i),y(i)) = (y^(i)−y(i))^2 / 2 
	L(y^(i),y(i)) = −(y(i)log(y^(i)) + (1−y(i))log(1−y^(i)))
成本函数: 
	1.全体训练样本上的表现	
	2.成本函数是整个训练集损失函数的平均值。我们试着去找到参数w和b使得成本函数最小化。
	3.成本函数是一个凸曲面函数
	J(w,b)=[∑(m,i=1) L(y^(i),y(i))] /m = [∑(m,i=1)(y(i))log(y^(i))+(1−y(i))log(1−y^(i))] /m
▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁			
梯度下降法
		从初始点开始,槽最陡的下坡方向走一步
	步长（Learning rate）：
		步长又称学习率，决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度。
		用上面下山的例子，步长就是在当前这一步所在位置沿着最陡峭最易下山的位置走的那一步的长度。
2.3 梯度下降法调优
	1.在梯度下降法中调优比较重要的是3个因素，步长、初始值、归一化。
	2.步长：步长太小，收敛慢，步长太大，会远离最优解。所以需要从小到大，分别测试，选出一个最优解。
	3.初始值：随机选取初始值，当损失函数是非凸函数时，找到的解可能是局部最优解，需要多测试几次，从局部最优解中选出最优解。当损失函数是凸函数时，得到的解就是最优解。
	4.归一化：如果不归一化，会收敛的很慢，会形成之字的路线。
	5.函数的斜率,是朝着下降熟读最快的方向走: w = w - dJ(w)/dw
	w = w - dJ(w,b)/dw
	b = b - dJ(w,b)/db
方向导数与梯度：
	1.导数反应的是函数的变化率
	2.而偏导数反应的是函数沿坐标轴方向的变化率
	3.偏导数 f'x(x0,y0) 表示固定面上一点对 x 轴的切线斜率；偏导数 f'y(x0,y0) 表示固定面上一点对 y 轴的切线斜率。
导数就是斜率	


	
	